{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7_V7DhLdoXL0",
        "4wCwjmzxuKyk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e45eff72",
        "outputId": "6ed139c6-0cb4-481f-f4dc-04e8645c6dd4"
      },
      "source": [
        "!pip install \\\n",
        "    \"fastapi>=0.128.0\" \\\n",
        "    \"langchain>=1.2.3\" \\\n",
        "    \"langchain-openai>=1.1.7\" \\\n",
        "    \"langgraph>=1.0.5\" \\\n",
        "    \"pydantic-settings>=2.12.0\" \\\n",
        "    \"python-dotenv>=1.2.1\" \\\n",
        "    \"uvicorn>=0.40.0\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi>=0.128.0\n",
            "  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: langchain>=1.2.3 in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Collecting langchain-openai>=1.1.7\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langgraph>=1.0.5 in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Requirement already satisfied: pydantic-settings>=2.12.0 in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: python-dotenv>=1.2.1 in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: uvicorn>=0.40.0 in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.128.0) (0.50.0)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.128.0) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.128.0) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.128.0) (0.0.4)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.2.3) (1.2.6)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=1.1.7) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=1.1.7) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.5) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.5) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.5) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.5) (3.6.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.12.0) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.40.0) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.40.0) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (0.13.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph>=1.0.5) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph>=1.0.5) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph>=1.0.5) (3.11.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=1.1.7) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=1.1.7) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=1.1.7) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=1.1.7) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=1.1.7) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->fastapi>=0.128.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->fastapi>=0.128.0) (2.41.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=1.1.7) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=1.1.7) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai>=1.1.7) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=1.0.5) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph>=1.0.5) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain>=1.2.3) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai>=1.1.7) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai>=1.1.7) (2.5.0)\n",
            "Downloading fastapi-0.128.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastapi, langchain-openai\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.123.10\n",
            "    Uninstalling fastapi-0.123.10:\n",
            "      Successfully uninstalled fastapi-0.123.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires fastapi<0.124.0,>=0.115.0, but you have fastapi 0.128.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.128.0 langchain-openai-1.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ApryvdEEmtoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from typing_extensions import TypedDict\n",
        "from typing import List, Dict, Optional, TypedDict, Annotated\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents.structured_output import ToolStrategy\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import operator\n",
        "import logging\n",
        "from pprint import pprint\n"
      ],
      "metadata": {
        "id": "ZjCrcJDMS8mM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent State"
      ],
      "metadata": {
        "id": "JL8BMBwXnfqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswersSoFar(TypedDict):\n",
        "    questions: Annotated[list[str], operator.add]\n",
        "    answers:   Annotated[list[str], operator.add]\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    initial_description: str\n",
        "    answers_so_far: AnswersSoFar\n",
        "    iteration_count: int\n",
        "    is_ready: bool               # set by question node when is_complete=True\n",
        "    final_description: Optional[str]\n",
        "    is_complete: bool            # final node sets this to True\n",
        "    error: Optional[str]         # both nodes can write here"
      ],
      "metadata": {
        "id": "bcoBEEfIniUm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts"
      ],
      "metadata": {
        "id": "7_V7DhLdoXL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.utils.input import print_text"
      ],
      "metadata": {
        "id": "OfCKXXkHq_3k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for asking clarifying questions\n",
        "ASK_QUESTIONS_PROMPT = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\",\n",
        "            \"\"\"You are an AI legal intake assistant. Your role is to gather information about a user's legal situation by asking clarifying questions.\n",
        "\n",
        "            The user has provided this initial description:\n",
        "            {initial_description}\n",
        "\n",
        "            Previous Questions Asked:\n",
        "            {previous_questions}\n",
        "\n",
        "            Previous answers from the user:\n",
        "            {previous_answers}\n",
        "\n",
        "            Your task is to ask 1-3 specific, relevant questions that will help clarify the legal situation. Focus on gathering facts that would be important for a lawyer to know.\n",
        "\n",
        "            Provide your response in the following JSON format:\n",
        "            {{\n",
        "                \"reasoning\": \"Brief explanation of why you're asking these questions\",\n",
        "                \"questions\": [\"question 1\", \"question 2\", \"question 3\"],\n",
        "                \"is_complete\": false\n",
        "            }}\n",
        "\n",
        "            IMPORTANT:\n",
        "            - Ask specific, factual questions\n",
        "            - Do not provide legal advice\n",
        "            - Do not make assumptions about the law\n",
        "            - If you have sufficient information to create a comprehensive case summary, set is_complete to true\n",
        "            - Limit to 3 questions max to avoid overwhelming the user\n",
        "            - Make sure questions are directly related to the legal matter at hand\n",
        "            \"\"\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "c6L9FiDtoW_3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = ASK_QUESTIONS_PROMPT.invoke(\n",
        "    {\n",
        "        'previous_answers': \"No Answers provided yet.\",\n",
        "        'previous_questions': \"No Questions provided yet.\",\n",
        "        'initial_description': \"I want to apply for a deviorce\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print_text(results.messages[-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_5EEsFxoW9J",
        "outputId": "3b0f70d6-f4f4-47d2-9cef-c98e6b3fd297"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI legal intake assistant. Your role is to gather information about a user's legal situation by asking clarifying questions.\n",
            "\n",
            "            The user has provided this initial description:\n",
            "            I want to apply for a deviorce\n",
            "\n",
            "            Previous Questions Asked:\n",
            "            No Questions provided yet.\n",
            "\n",
            "            Previous answers from the user:\n",
            "            No Answers provided yet.\n",
            "\n",
            "            Your task is to ask 1-3 specific, relevant questions that will help clarify the legal situation. Focus on gathering facts that would be important for a lawyer to know.\n",
            "\n",
            "            Provide your response in the following JSON format:\n",
            "            {\n",
            "                \"reasoning\": \"Brief explanation of why you're asking these questions\",\n",
            "                \"questions\": [\"question 1\", \"question 2\", \"question 3\"],\n",
            "                \"is_complete\": false\n",
            "            }\n",
            "\n",
            "            IMPORTANT:\n",
            "            - Ask specific, factual questions\n",
            "            - Do not provide legal advice\n",
            "            - Do not make assumptions about the law\n",
            "            - If you have sufficient information to create a comprehensive case summary, set is_complete to true\n",
            "            - Limit to 3 questions max to avoid overwhelming the user\n",
            "            - Make sure questions are directly related to the legal matter at hand\n",
            "            "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Structured output format for question-asking\n",
        "STRUCTURED_QUESTION_RESPONSE_FORMAT = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"reasoning\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Brief explanation of why you're asking these questions\"\n",
        "        },\n",
        "        \"questions\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "            \"description\": \"1-3 specific, relevant questions to ask the user\"\n",
        "        },\n",
        "        \"is_complete\": {\n",
        "            \"type\": \"boolean\",\n",
        "            \"description\": \"True if you have enough information to create a final description, false otherwise\"\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"reasoning\", \"questions\", \"is_complete\"]\n",
        "}"
      ],
      "metadata": {
        "id": "sNSHaoJToW3o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FINALIZE_DESCRIPTION_PROMPT = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\",\"\"\"\n",
        "You are an AI legal assistant. Based on the following information, create a professional, comprehensive case description that a lawyer could use to understand the situation:\n",
        "\n",
        "Initial description: {initial_description}\n",
        "\n",
        "Questions and answers:\n",
        "  Questions:\n",
        "    {all_questions}\n",
        "  Answers:\n",
        "    {all_answers}\n",
        "\n",
        "Your task is to synthesize this information into a clear, professional case summary that includes:\n",
        "1. The key facts of the situation\n",
        "2. The legal issues involved (without offering legal advice)\n",
        "3. Any relevant timelines or important details\n",
        "4. The client's apparent goals or concerns\n",
        "\n",
        "Format your response as a well-structured professional summary. Be thorough but concise. Do NOT provide legal advice or recommendations. Simply summarize the facts as presented.\n",
        "\n",
        "IMPORTANT DISCLAIMERS TO INCLUDE AT THE END:\n",
        "This is an AI-generated summary based solely on the information provided. It is NOT legal advice. Consult a qualified attorney for legal guidance.\n",
        "\"\"\"\n",
        "      )\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "HyK_AhDtoW6L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nodes"
      ],
      "metadata": {
        "id": "hdAQc-x4uD4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "HKX3CW-ku9dp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    api_key=userdata.get('OPENROUTER_API_KEY'),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    model=\"x-ai/grok-4.1-fast\",\n",
        ")"
      ],
      "metadata": {
        "id": "6sTOIh7ZoW1N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_state = AgentState(\n",
        "    initial_description=\"I want to apply for a divorce\",\n",
        "    iteration_count=0,\n",
        "    answers_so_far={'questions': [], 'answers': []},\n",
        "    is_ready=False,\n",
        "    final_description=None,\n",
        "    is_complete=False,\n",
        "    error=None\n",
        ")"
      ],
      "metadata": {
        "id": "a50ijevwvn8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## @node -- Ask Questions Node --"
      ],
      "metadata": {
        "id": "Yf6hIb9ovadC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-21m14L2JEZ",
        "outputId": "773fdac8-5174-4fce-9066-e1cf2b5b0b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_description': 'I want to apply for a divorce',\n",
              " 'iteration_count': 0,\n",
              " 'answers_so_far': {'questions': [], 'answers': []},\n",
              " 'is_ready': False,\n",
              " 'final_description': None,\n",
              " 'is_complete': False}"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_state['answers_so_far']['answers']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCftoH8iKGeh",
        "outputId": "92f06028-0cd5-4c3c-8ec1-21a90ace40a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No Answers provided yet.']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_questions(state: AgentState) -> AgentState:\n",
        "\n",
        "  print(\"---Executing Ask Questions Node---\")\n",
        "  if not state['answers_so_far']['answers']:\n",
        "    state['answers_so_far']['answers'].append(\"No Answers provided yet.\")\n",
        "    state['answers_so_far']['questions'].append(\"No Questions provided yet.\")\n",
        "\n",
        "  prompt = ASK_QUESTIONS_PROMPT.invoke(\n",
        "      {\n",
        "          'initial_description': state['initial_description'],\n",
        "          'previous_answers': state['answers_so_far']['answers'],\n",
        "          'previous_questions': state['answers_so_far']['questions'],\n",
        "      })\n",
        "\n",
        "  strategy = ToolStrategy(STRUCTURED_QUESTION_RESPONSE_FORMAT)\n",
        "\n",
        "  agent = create_agent(\n",
        "        llm,\n",
        "        tools=[],\n",
        "        system_prompt=prompt.messages[-1].content,\n",
        "        response_format=strategy,\n",
        "    )\n",
        "\n",
        "  try:\n",
        "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \" \"}]})\n",
        "    # FIX: Use extend instead of append to add individual questions from the list\n",
        "    for question in result[\"structured_response\"][\"questions\"]:\n",
        "      state['answers_so_far']['questions'].append(question)\n",
        "  except Exception as e:\n",
        "    print(f'üö© Error adding questions: {e}')\n",
        "    return state\n",
        "\n",
        "  state['iteration_count'] += 1\n",
        "  # This line removes the initial placeholder, ensuring the list only contains actual questions.\n",
        "  state['answers_so_far']['questions'].remove(\"No Questions provided yet.\")\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "eBlvzvu7uGqo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_state = ask_questions(init_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLMrgeusEGGU",
        "outputId": "f5d16d67-471d-43eb-985d-6213676fb4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Executing Ask Questions Node---\n",
            "True\n",
            "{'reasoning': 'To determine the jurisdiction for filing the divorce, identify if there are minor children involved which affects custody and support, and gather basic marriage details relevant to the process.', 'questions': ['In which state, province, or country do you and your spouse currently reside?', 'Do you have any children together, and if so, how old are they?', 'When and where were you married?'], 'is_complete': False}\n",
            "\n",
            " Questions: ['In which state, province, or country do you and your spouse currently reside?', 'Do you have any children together, and if so, how old are they?', 'When and where were you married?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6HR4QU1LzHU",
        "outputId": "a01e1a99-d228-4333-e6f9-5a955d28e204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_description': 'I want to apply for a divorce',\n",
              " 'iteration_count': 1,\n",
              " 'answers_so_far': {'questions': ['In which state, province, or country do you and your spouse currently reside?',\n",
              "   'Do you have any children together, and if so, how old are they?',\n",
              "   'When and where were you married?'],\n",
              "  'answers': ['No Answers provided yet.']},\n",
              " 'is_ready': False,\n",
              " 'final_description': None,\n",
              " 'is_complete': False}"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## @node -- Ask Questions V2 --"
      ],
      "metadata": {
        "id": "gPIvBD3_TGHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def generate_questions_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Generates 1‚Äì3 new questions or decides we're ready.\n",
        "    Uses create_react_agent so you can later plug in real tools (RAG/legal lookup).\n",
        "    Pure node ‚Üí returns delta only.\n",
        "    \"\"\"\n",
        "    if state.get(\"is_ready\", False) or state.get(\"is_complete\", False):\n",
        "        return {}  # already done\n",
        "\n",
        "    # Build agent (you can move this outside / inject if preferred)\n",
        "    prompt = ASK_QUESTIONS_PROMPT.invoke(\n",
        "      {\n",
        "          \"initial_description\": state['initial_description'],\n",
        "          \"previous_questions\": \"\\n- \" + \"\\n- \".join(state[\"answers_so_far\"][\"questions\"]) if state[\"answers_so_far\"][\"questions\"] else \"None yet.\",\n",
        "          \"previous_answers\":   \"\\n- \" + \"\\n- \".join(state[\"answers_so_far\"][\"answers\"])   if state[\"answers_so_far\"][\"answers\"] else \"None provided.\",\n",
        "      })\n",
        "\n",
        "    agent = create_agent(\n",
        "          llm,\n",
        "          tools=[],\n",
        "          system_prompt=prompt.messages[-1].content,\n",
        "          response_format=ToolStrategy(STRUCTURED_QUESTION_RESPONSE_FORMAT),\n",
        "      )\n",
        "\n",
        "    try:\n",
        "        parsed = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Please continue the intake process now.\"}]})[\"structured_response\"]\n",
        "\n",
        "        new_questions = parsed.get(\"questions\", [])\n",
        "        reasoning = parsed.get(\"reasoning\", \"\")\n",
        "        is_complete_now = parsed.get(\"is_complete\", False)\n",
        "\n",
        "        update = {\n",
        "            \"answers_so_far\": {\n",
        "                \"questions\": new_questions,   # auto-appends thanks to reducer\n",
        "            },\n",
        "            \"iteration_count\": state[\"iteration_count\"] + 1,\n",
        "            \"is_ready\": is_complete_now,      # question node can set this\n",
        "            \"is_complete\": False,             # only final node sets true\n",
        "        }\n",
        "\n",
        "        if is_complete_now:\n",
        "            logger.info(f\"Questions complete after {update['iteration_count']} iterations\")\n",
        "        else:\n",
        "            logger.info(f\"Asking {len(new_questions)} more questions (iter {update['iteration_count']})\")\n",
        "\n",
        "        return update\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Question generation failed\")\n",
        "        return {\n",
        "            \"error\": f\"Question generation failed: {str(e)}\",\n",
        "            \"iteration_count\": state[\"iteration_count\"] + 1,\n",
        "        }"
      ],
      "metadata": {
        "id": "dECYIOGvTF-n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO5qoVvGTF7p",
        "outputId": "67889c2c-5136-4855-9d6f-8f45c3bf410e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_description': 'I want to apply for a divorce',\n",
              " 'iteration_count': 0,\n",
              " 'answers_so_far': {'questions': [], 'answers': []},\n",
              " 'is_ready': False,\n",
              " 'final_description': None,\n",
              " 'is_complete': False,\n",
              " 'error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_questions_node(init_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXeUtOr7TF4-",
        "outputId": "75472259-bdb7-4197-da41-cd9ef55ac6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers_so_far': {'questions': ['In which state or country do you currently reside, and for how long?',\n",
              "   'When and where were you married?',\n",
              "   'Do you have any children together, and if so, how old are they?']},\n",
              " 'iteration_count': 1,\n",
              " 'is_ready': False,\n",
              " 'is_complete': False}"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## @node -- Final Description Node --"
      ],
      "metadata": {
        "id": "7Osfd3a4Z5n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_description_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Only called when is_ready == True.\n",
        "    Produces the polished professional description.\n",
        "    Can also use agent/tools later (legal context, statutes...).\n",
        "    \"\"\"\n",
        "    if not state.get(\"is_ready\"):\n",
        "        return {\"error\": \"Final node called without is_ready=True\"}\n",
        "\n",
        "    prompt = FINALIZE_DESCRIPTION_PROMPT.invoke(\n",
        "      {\n",
        "          \"initial_description\": state['initial_description'],\n",
        "          \"all_questions\": \"\\n- \" + \"\\n- \".join(state[\"answers_so_far\"][\"questions\"]) if state[\"answers_so_far\"][\"questions\"] else \"None yet.\",\n",
        "          \"all_answers\":   \"\\n- \" + \"\\n- \".join(state[\"answers_so_far\"][\"answers\"])   if state[\"answers_so_far\"][\"answers\"] else \"None provided.\",\n",
        "      }).messages[-1].content\n",
        "    # print(f'Prompt: {prompt}')\n",
        "\n",
        "    try:\n",
        "        final_text = llm.invoke(prompt).content\n",
        "        print(f'Final Description: {final_text}')\n",
        "        return {\n",
        "            \"final_description\": final_text.content if hasattr(final_text, \"content\") else str(final_text),\n",
        "            \"is_complete\": True,\n",
        "            \"is_ready\": False,  # cleanup\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Final description generation failed: {str(e)}\"}"
      ],
      "metadata": {
        "id": "jkwylk_zTF2N"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5abdb87c",
        "outputId": "ff2824c8-b461-4990-9ab2-a0fbd382ba67"
      },
      "source": [
        "# Create an initial state for testing generate_final_description_node\n",
        "test_final_state = AgentState(\n",
        "    initial_description=\"I want to apply for a divorce, my spouse and I have been separated for 6 months.\",\n",
        "    answers_so_far={\n",
        "        'questions': [\n",
        "            \"In which state, province, or country do you and your spouse currently reside?\",\n",
        "            \"Do you have any children together, and if so, how old are they?\",\n",
        "            \"When and where were you married?\"\n",
        "        ],\n",
        "        'answers': [\n",
        "            \"We both reside in California.\",\n",
        "            \"We have two children, ages 8 and 12.\",\n",
        "            \"We were married in Las Vegas, Nevada, in 2010.\"\n",
        "        ]\n",
        "    },\n",
        "    iteration_count=3,\n",
        "    is_ready=True, # Set to True to allow the final description node to execute\n",
        "    final_description=None,\n",
        "    is_complete=False,\n",
        "    error=None\n",
        ")\n",
        "\n",
        "print(\"Test final state created:\")\n",
        "print(test_final_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test final state created:\n",
            "{'initial_description': 'I want to apply for a divorce, my spouse and I have been separated for 6 months.', 'answers_so_far': {'questions': ['In which state, province, or country do you and your spouse currently reside?', 'Do you have any children together, and if so, how old are they?', 'When and where were you married?'], 'answers': ['We both reside in California.', 'We have two children, ages 8 and 12.', 'We were married in Las Vegas, Nevada, in 2010.']}, 'iteration_count': 3, 'is_ready': True, 'final_description': None, 'is_complete': False, 'error': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c54c45c3",
        "outputId": "a7dd0896-54b3-4e35-fcfc-f9fa0fe5fee0"
      },
      "source": [
        "# Call the generate_final_description_node with the test_final_state\n",
        "final_description_result = generate_final_description_node(test_final_state)\n",
        "\n",
        "print(\"\\nResult from generate_final_description_node:\")\n",
        "print(final_description_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "You are an AI legal assistant. Based on the following information, create a professional, comprehensive case description that a lawyer could use to understand the situation:\n",
            "\n",
            "Initial description: I want to apply for a divorce, my spouse and I have been separated for 6 months.\n",
            "\n",
            "Questions and answers:\n",
            "  Questions:\n",
            "    \n",
            "- In which state, province, or country do you and your spouse currently reside?\n",
            "- Do you have any children together, and if so, how old are they?\n",
            "- When and where were you married?\n",
            "  Answers:\n",
            "    \n",
            "- We both reside in California.\n",
            "- We have two children, ages 8 and 12.\n",
            "- We were married in Las Vegas, Nevada, in 2010.\n",
            "\n",
            "Your task is to synthesize this information into a clear, professional case summary that includes:\n",
            "1. The key facts of the situation\n",
            "2. The legal issues involved (without offering legal advice)\n",
            "3. Any relevant timelines or important details\n",
            "4. The client's apparent goals or concerns\n",
            "\n",
            "Format your response as a well-structured professional summary. Be thorough but concise. Do NOT provide legal advice or recommendations. Simply summarize the facts as presented.\n",
            "\n",
            "IMPORTANT DISCLAIMERS TO INCLUDE AT THE END:\n",
            "This is an AI-generated summary based solely on the information provided. It is NOT legal advice. Consult a qualified attorney for legal guidance.\n",
            "\n",
            "Final Description: # Case Summary: Divorce Inquiry\n",
            "\n",
            "## Client Overview\n",
            "- **Client's Stated Goal**: The client seeks to apply for a divorce from their spouse.\n",
            "- **Current Residence**: Both the client and spouse reside in California.\n",
            "\n",
            "## Key Facts\n",
            "- The client and spouse have been separated for 6 months.\n",
            "- The couple has two minor children together, ages 8 and 12.\n",
            "- Marriage occurred in Las Vegas, Nevada, in 2010 (approximately 14 years of marriage as of 2024).\n",
            "\n",
            "## Relevant Timeline\n",
            "- **Marriage Date and Location**: 2010, Las Vegas, Nevada.\n",
            "- **Separation Date**: Approximately 6 months prior to inquiry (exact date not specified).\n",
            "- **Current Status**: Parties separated; no additional details provided on assets, support, or living arrangements.\n",
            "\n",
            "## Legal Issues Involved\n",
            "- Jurisdiction and venue considerations based on residency in California and out-of-state marriage.\n",
            "- Potential matters related to dissolution of marriage, including division of marital property and debts.\n",
            "- Child-related issues, such as custody, visitation, and support for minor children ages 8 and 12.\n",
            "- Separation period and its implications for filing requirements.\n",
            "\n",
            "## Client's Apparent Goals or Concerns\n",
            "- Primary objective is to initiate divorce proceedings.\n",
            "- No specific concerns articulated beyond the desire to apply for divorce; child welfare may be implicit given presence of minors.\n",
            "\n",
            "**IMPORTANT DISCLAIMERS**:  \n",
            "This is an AI-generated summary based solely on the information provided. It is NOT legal advice. Consult a qualified attorney for legal guidance.\n",
            "\n",
            "Result from generate_final_description_node:\n",
            "{'final_description': \"# Case Summary: Divorce Inquiry\\n\\n## Client Overview\\n- **Client's Stated Goal**: The client seeks to apply for a divorce from their spouse.\\n- **Current Residence**: Both the client and spouse reside in California.\\n\\n## Key Facts\\n- The client and spouse have been separated for 6 months.\\n- The couple has two minor children together, ages 8 and 12.\\n- Marriage occurred in Las Vegas, Nevada, in 2010 (approximately 14 years of marriage as of 2024).\\n\\n## Relevant Timeline\\n- **Marriage Date and Location**: 2010, Las Vegas, Nevada.\\n- **Separation Date**: Approximately 6 months prior to inquiry (exact date not specified).\\n- **Current Status**: Parties separated; no additional details provided on assets, support, or living arrangements.\\n\\n## Legal Issues Involved\\n- Jurisdiction and venue considerations based on residency in California and out-of-state marriage.\\n- Potential matters related to dissolution of marriage, including division of marital property and debts.\\n- Child-related issues, such as custody, visitation, and support for minor children ages 8 and 12.\\n- Separation period and its implications for filing requirements.\\n\\n## Client's Apparent Goals or Concerns\\n- Primary objective is to initiate divorce proceedings.\\n- No specific concerns articulated beyond the desire to apply for divorce; child welfare may be implicit given presence of minors.\\n\\n**IMPORTANT DISCLAIMERS**:  \\nThis is an AI-generated summary based solely on the information provided. It is NOT legal advice. Consult a qualified attorney for legal guidance.\", 'is_complete': True, 'is_ready': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## @ConditionalEdge -- Route After Questions --"
      ],
      "metadata": {
        "id": "GeySUhOviL0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END"
      ],
      "metadata": {
        "id": "dRs2T-r1imRw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After questions ‚Üí decide\n",
        "def route_after_questions(state: AgentState) -> str:\n",
        "    if state.get(\"error\"):\n",
        "        return END\n",
        "    if state.get(\"is_ready\"):\n",
        "        return \"generate_final\"\n",
        "    if state[\"iteration_count\"] >= 3:  # safety\n",
        "        return \"generate_final\"   # or END with partial\n",
        "    return \"generate_questions\"       # loop back\n",
        "\n",
        "# Dummy node\n",
        "def get_answers(state: AgentState):\n",
        "  pass"
      ],
      "metadata": {
        "id": "ABAspuEqTFwo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_after_questions(test_final_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OU7_Mr8hTFuA",
        "outputId": "4c4de442-4eaa-459d-b9d5-11ee44d12473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'generate_final'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph Workflow"
      ],
      "metadata": {
        "id": "4yGgd5BIi2fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=AgentState)\n",
        "\n",
        "workflow.add_node(\"generate_questions\", generate_questions_node)\n",
        "workflow.add_node(\"generate_final\", generate_final_description_node)\n",
        "workflow.add_node(\"get_answers\", get_answers)\n",
        "\n",
        "# Entry\n",
        "workflow.add_edge(START, \"generate_questions\")\n",
        "workflow.add_edge(\"generate_questions\", \"get_answers\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"get_answers\",\n",
        "    route_after_questions,\n",
        "    {\n",
        "        \"generate_questions\": \"generate_questions\",\n",
        "        \"generate_final\": \"generate_final\",\n",
        "        END: END\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"generate_final\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(interrupt_before=[\"get_answers\"], checkpointer=memory)"
      ],
      "metadata": {
        "id": "0c5GwV9Ni72L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image, Markdown"
      ],
      "metadata": {
        "id": "tTU05u6MkDAg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "dujt1TXwi70A",
        "outputId": "75f0642b-f655-4527-d69e-598ff66457b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_state = AgentState(\n",
        "    initial_description=\"I want to apply for a divorce\",\n",
        "    iteration_count=0,\n",
        "    answers_so_far={'questions': [], 'answers': []},\n",
        "    is_ready=False,\n",
        "    final_description=None,\n",
        "    is_complete=False,\n",
        "    error=None\n",
        ")"
      ],
      "metadata": {
        "id": "pIg5rf5wi7w4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "result = graph.invoke(init_state, config=config)\n",
        "result"
      ],
      "metadata": {
        "id": "qw0npuPTi7ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e242e75-9216-4596-e5c0-113440b3caac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_description': 'I want to apply for a divorce',\n",
              " 'answers_so_far': {'questions': ['In which state or country do you currently reside?',\n",
              "   'Where and when were you legally married?',\n",
              "   'Do you have any children together, and if so, how old are they?']},\n",
              " 'iteration_count': 1,\n",
              " 'is_ready': False,\n",
              " 'final_description': None,\n",
              " 'is_complete': False,\n",
              " 'error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = graph.get_state(config)\n",
        "state.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60lARXEX1V4A",
        "outputId": "be572353-6515-4e20-c5cb-4b36ee8a5907"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_description': 'I want to apply for a divorce',\n",
              " 'answers_so_far': {'questions': ['In which state or country do you currently reside?',\n",
              "   'Where and when were you legally married?',\n",
              "   'Do you have any children together, and if so, how old are they?']},\n",
              " 'iteration_count': 1,\n",
              " 'is_ready': False,\n",
              " 'final_description': None,\n",
              " 'is_complete': False,\n",
              " 'error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state.values['answers_so_far']['questions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZiLXADbGw6J",
        "outputId": "0f18c6ef-1e9e-4310-93ea-a8eefdf13d95"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In which state (or country, if outside the US) do you and your spouse currently reside?',\n",
              " 'Approximately how long have you been married, and what is the date of your marriage?',\n",
              " 'Do you have any minor children (under 18) with your spouse, and if so, how many?']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_state = graph.get_state(config)\n",
        "existing_questions = current_state.values['answers_so_far']['questions']\n",
        "\n",
        "graph.update_state(config,\n",
        " {\n",
        "    \"answers_so_far\": {\n",
        "        'questions': existing_questions,\n",
        "        'answers': [\n",
        "            'We both reside in California.',\n",
        "            'We were married oct/2010 in Las Vegas',\n",
        "            'We have two children, ages 8 and 12.'\n",
        "        ]\n",
        "    },\n",
        "    \"iteration_count\": 1,\n",
        "    \"is_ready\": True\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3IaQTuL1vU-",
        "outputId": "921b4ff3-8423-43d3-80b9-3bcd283fac1c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1f0f136a-6042-69f7-8006-f3e758beb98d'}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.get_state(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTr3X_fgFta3",
        "outputId": "f31e49da-f8f5-4a38-9c19-3b17ceaffc0d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'initial_description': 'I want to apply for a divorce', 'answers_so_far': {'questions': ['In which state or country do you currently reside?', 'Where and when were you legally married?', 'Do you have any children together, and if so, how old are they?'], 'answers': ['We both reside in California.', 'We were married oct/2010 in Las Vegas', 'We have two children, ages 8 and 12.']}, 'iteration_count': 1, 'is_ready': True, 'final_description': None, 'is_complete': False, 'error': None}, next=('get_answers',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0f136a-6042-69f7-8006-f3e758beb98d'}}, metadata={'source': 'update', 'step': 6, 'parents': {}}, created_at='2026-01-14T10:49:06.430384+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0f1362-7eab-62de-8005-5346c4f79ecd'}}, tasks=(PregelTask(id='dca8037e-5382-8027-7954-52aa900bd173', name='get_answers', path=('__pregel_pull', 'get_answers'), error=None, interrupts=(), state=None, result=None),), interrupts=())"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resumed_results = graph.invoke(None, config=config)\n",
        "resumed_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-gX102fGbWH",
        "outputId": "3634ba6a-d5fc-4052-d5fd-eb497b6d9173"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial_description': 'I want to apply for a divorce',\n",
              " 'answers_so_far': {'questions': ['In which state or country do you currently reside?',\n",
              "   'Where and when were you legally married?',\n",
              "   'Do you have any children together, and if so, how old are they?'],\n",
              "  'answers': ['We both reside in California.',\n",
              "   'We were married oct/2010 in Las Vegas',\n",
              "   'We have two children, ages 8 and 12.']},\n",
              " 'iteration_count': 1,\n",
              " 'is_ready': False,\n",
              " 'final_description': \"# Case Summary: Divorce Inquiry\\n\\n## Client and Key Facts\\n- **Client's Stated Goal**: The client seeks to apply for a divorce from their spouse.\\n- **Parties Involved**: The client and their spouse, with two minor children in common.\\n- **Residence**: Both spouses currently reside in California.\\n- **Children**: Two children, aged 8 and 12.\\n\\n## Marital History\\n- **Date and Location of Marriage**: Legally married in October 2010 in Las Vegas (Nevada).\\n\\n## Timeline of Relevant Events\\n- Marriage: October 2010 (approximately 14 years ago).\\n- Children: Ages indicate births around 2012 and 2016.\\n- Current Status: Client initiating divorce proceedings as of the inquiry date.\\n\\n## Potential Legal Issues\\n- Dissolution of marriage, given the California residency of both parties.\\n- Matters related to minor children, including custody, visitation, and support.\\n- Jurisdiction considerations due to out-of-state marriage location (Nevada).\\n\\n## Client's Apparent Goals or Concerns\\n- Primary objective: Filing for divorce.\\n- No additional concerns explicitly stated beyond the desire to initiate divorce proceedings.\\n\\n**IMPORTANT DISCLAIMERS:**  \\nThis is an AI-generated summary based solely on the information provided. It is NOT legal advice. Consult a qualified attorney for legal guidance.\",\n",
              " 'is_complete': True,\n",
              " 'error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(resumed_results['final_description']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "Cd4Vdq6FIf59",
        "outputId": "c4d478cb-364e-46d9-8dc0-b58ce61b827d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Case Summary: Divorce Inquiry\n\n## Client and Key Facts\n- **Client's Stated Goal**: The client seeks to apply for a divorce from their spouse.\n- **Parties Involved**: The client and their spouse, with two minor children in common.\n- **Residence**: Both spouses currently reside in California.\n- **Children**: Two children, aged 8 and 12.\n\n## Marital History\n- **Date and Location of Marriage**: Legally married in October 2010 in Las Vegas (Nevada).\n\n## Timeline of Relevant Events\n- Marriage: October 2010 (approximately 14 years ago).\n- Children: Ages indicate births around 2012 and 2016.\n- Current Status: Client initiating divorce proceedings as of the inquiry date.\n\n## Potential Legal Issues\n- Dissolution of marriage, given the California residency of both parties.\n- Matters related to minor children, including custody, visitation, and support.\n- Jurisdiction considerations due to out-of-state marriage location (Nevada).\n\n## Client's Apparent Goals or Concerns\n- Primary objective: Filing for divorce.\n- No additional concerns explicitly stated beyond the desire to initiate divorce proceedings.\n\n**IMPORTANT DISCLAIMERS:**  \nThis is an AI-generated summary based solely on the information provided. It is NOT legal advice. Consult a qualified attorney for legal guidance."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assets"
      ],
      "metadata": {
        "id": "4wCwjmzxuKyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ChatOpenAI.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT2elC7yggF7",
        "outputId": "4665d4d4-7bbd-41a8-84fc-e55708347376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interface to OpenAI chat model APIs.\n",
            "\n",
            "    ???+ info \"Setup\"\n",
            "\n",
            "        Install `langchain-openai` and set environment variable `OPENAI_API_KEY`.\n",
            "\n",
            "        ```bash\n",
            "        pip install -U langchain-openai\n",
            "\n",
            "        # or using uv\n",
            "        uv add langchain-openai\n",
            "        ```\n",
            "\n",
            "        ```bash\n",
            "        export OPENAI_API_KEY=\"your-api-key\"\n",
            "        ```\n",
            "\n",
            "    ??? info \"Key init args ‚Äî completion params\"\n",
            "\n",
            "        | Param               | Type          | Description                                                                                                 |\n",
            "        | ------------------- | ------------- | ----------------------------------------------------------------------------------------------------------- |\n",
            "        | `model`             | `str`         | Name of OpenAI model to use.                                                                                |\n",
            "        | `temperature`       | `float`       | Sampling temperature.                                                                                       |\n",
            "        | `max_tokens`        | `int | None`  | Max number of tokens to generate.                                                                           |\n",
            "        | `logprobs`          | `bool | None` | Whether to return logprobs.                                                                                 |\n",
            "        | `stream_options`    | `dict`        | Configure streaming outputs, like whether to return token usage when streaming (`{\"include_usage\": True}`). |\n",
            "        | `use_responses_api` | `bool | None` | Whether to use the responses API.                                                                           |\n",
            "\n",
            "        See full list of supported init args and their descriptions below.\n",
            "\n",
            "    ??? info \"Key init args ‚Äî client params\"\n",
            "\n",
            "        | Param          | Type                                       | Description                                                                         |\n",
            "        | -------------- | ------------------------------------------ | ----------------------------------------------------------------------------------- |\n",
            "        | `timeout`      | `float | Tuple[float, float] | Any | None` | Timeout for requests.                                                               |\n",
            "        | `max_retries`  | `int | None`                               | Max number of retries.                                                              |\n",
            "        | `api_key`      | `str | None`                               | OpenAI API key. If not passed in will be read from env var `OPENAI_API_KEY`.        |\n",
            "        | `base_url`     | `str | None`                               | Base URL for API requests. Only specify if using a proxy or service emulator.       |\n",
            "        | `organization` | `str | None`                               | OpenAI organization ID. If not passed in will be read from env var `OPENAI_ORG_ID`. |\n",
            "\n",
            "        See full list of supported init args and their descriptions below.\n",
            "\n",
            "    ??? info \"Instantiate\"\n",
            "\n",
            "        Create a model instance with desired params. For example:\n",
            "\n",
            "        ```python\n",
            "        from langchain_openai import ChatOpenAI\n",
            "\n",
            "        model = ChatOpenAI(\n",
            "            model=\"...\",\n",
            "            temperature=0,\n",
            "            max_tokens=None,\n",
            "            timeout=None,\n",
            "            max_retries=2,\n",
            "            # api_key=\"...\",\n",
            "            # base_url=\"...\",\n",
            "            # organization=\"...\",\n",
            "            # other params...\n",
            "        )\n",
            "        ```\n",
            "\n",
            "        See all available params below.\n",
            "\n",
            "        !!! tip \"Preserved params\"\n",
            "            Any param which is not explicitly supported will be passed directly to\n",
            "            [`openai.OpenAI.chat.completions.create(...)`](https://platform.openai.com/docs/api-reference/chat/create)\n",
            "            every time to the model is invoked. For example:\n",
            "\n",
            "            ```python\n",
            "            from langchain_openai import ChatOpenAI\n",
            "            import openai\n",
            "\n",
            "            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n",
            "\n",
            "            # Results in underlying API call of:\n",
            "\n",
            "            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n",
            "\n",
            "            # Which is also equivalent to:\n",
            "\n",
            "            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n",
            "            ```\n",
            "\n",
            "    ??? info \"Invoke\"\n",
            "\n",
            "        Generate a response from the model:\n",
            "\n",
            "        ```python\n",
            "        messages = [\n",
            "            (\n",
            "                \"system\",\n",
            "                \"You are a helpful translator. Translate the user sentence to French.\",\n",
            "            ),\n",
            "            (\"human\", \"I love programming.\"),\n",
            "        ]\n",
            "        model.invoke(messages)\n",
            "        ```\n",
            "\n",
            "        Results in an `AIMessage` response:\n",
            "\n",
            "        ```python\n",
            "        AIMessage(\n",
            "            content=\"J'adore la programmation.\",\n",
            "            response_metadata={\n",
            "                \"token_usage\": {\n",
            "                    \"completion_tokens\": 5,\n",
            "                    \"prompt_tokens\": 31,\n",
            "                    \"total_tokens\": 36,\n",
            "                },\n",
            "                \"model_name\": \"gpt-4o\",\n",
            "                \"system_fingerprint\": \"fp_43dfabdef1\",\n",
            "                \"finish_reason\": \"stop\",\n",
            "                \"logprobs\": None,\n",
            "            },\n",
            "            id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
            "            usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
            "        )\n",
            "        ```\n",
            "\n",
            "    ??? info \"Stream\"\n",
            "\n",
            "        Stream a response from the model:\n",
            "\n",
            "        ```python\n",
            "        for chunk in model.stream(messages):\n",
            "            print(chunk.text, end=\"\")\n",
            "        ```\n",
            "\n",
            "        Results in a sequence of `AIMessageChunk` objects with partial content:\n",
            "\n",
            "        ```python\n",
            "        AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
            "        AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
            "        AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
            "        AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
            "        AIMessageChunk(\n",
            "            content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n",
            "        )\n",
            "        AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
            "        AIMessageChunk(\n",
            "            content=\"\",\n",
            "            response_metadata={\"finish_reason\": \"stop\"},\n",
            "            id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n",
            "        )\n",
            "        ```\n",
            "\n",
            "        To collect the full message, you can concatenate the chunks:\n",
            "\n",
            "        ```python\n",
            "        stream = model.stream(messages)\n",
            "        full = next(stream)\n",
            "        for chunk in stream:\n",
            "            full += chunk\n",
            "        ```\n",
            "\n",
            "        ```python\n",
            "        full = AIMessageChunk(\n",
            "            content=\"J'adore la programmation.\",\n",
            "            response_metadata={\"finish_reason\": \"stop\"},\n",
            "            id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n",
            "        )\n",
            "        ```\n",
            "\n",
            "    ??? info \"Async\"\n",
            "\n",
            "        Asynchronous equivalents of `invoke`, `stream`, and `batch` are also available:\n",
            "\n",
            "        ```python\n",
            "        # Invoke\n",
            "        await model.ainvoke(messages)\n",
            "\n",
            "        # Stream\n",
            "        async for chunk in (await model.astream(messages))\n",
            "\n",
            "        # Batch\n",
            "        await model.abatch([messages])\n",
            "        ```\n",
            "\n",
            "        Results in an `AIMessage` response:\n",
            "\n",
            "        ```python\n",
            "        AIMessage(\n",
            "            content=\"J'adore la programmation.\",\n",
            "            response_metadata={\n",
            "                \"token_usage\": {\n",
            "                    \"completion_tokens\": 5,\n",
            "                    \"prompt_tokens\": 31,\n",
            "                    \"total_tokens\": 36,\n",
            "                },\n",
            "                \"model_name\": \"gpt-4o\",\n",
            "                \"system_fingerprint\": \"fp_43dfabdef1\",\n",
            "                \"finish_reason\": \"stop\",\n",
            "                \"logprobs\": None,\n",
            "            },\n",
            "            id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
            "            usage_metadata={\n",
            "                \"input_tokens\": 31,\n",
            "                \"output_tokens\": 5,\n",
            "                \"total_tokens\": 36,\n",
            "            },\n",
            "        )\n",
            "        ```\n",
            "\n",
            "        For batched calls, results in a `list[AIMessage]`.\n",
            "\n",
            "    ??? info \"Tool calling\"\n",
            "\n",
            "        ```python\n",
            "        from pydantic import BaseModel, Field\n",
            "\n",
            "\n",
            "        class GetWeather(BaseModel):\n",
            "            '''Get the current weather in a given location'''\n",
            "\n",
            "            location: str = Field(\n",
            "                ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
            "            )\n",
            "\n",
            "\n",
            "        class GetPopulation(BaseModel):\n",
            "            '''Get the current population in a given location'''\n",
            "\n",
            "            location: str = Field(\n",
            "                ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
            "            )\n",
            "\n",
            "\n",
            "        model_with_tools = model.bind_tools(\n",
            "            [GetWeather, GetPopulation]\n",
            "            # strict = True  # Enforce tool args schema is respected\n",
            "        )\n",
            "        ai_msg = model_with_tools.invoke(\n",
            "            \"Which city is hotter today and which is bigger: LA or NY?\"\n",
            "        )\n",
            "        ai_msg.tool_calls\n",
            "        ```\n",
            "\n",
            "        ```python\n",
            "        [\n",
            "            {\n",
            "                \"name\": \"GetWeather\",\n",
            "                \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            "                \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"GetWeather\",\n",
            "                \"args\": {\"location\": \"New York, NY\"},\n",
            "                \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"GetPopulation\",\n",
            "                \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            "                \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"GetPopulation\",\n",
            "                \"args\": {\"location\": \"New York, NY\"},\n",
            "                \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n",
            "            },\n",
            "        ]\n",
            "        ```\n",
            "\n",
            "        !!! note \"Parallel tool calls\"\n",
            "            [`openai >= 1.32`](https://pypi.org/project/openai/) supports a\n",
            "            `parallel_tool_calls` parameter that defaults to `True`. This parameter can\n",
            "            be set to `False` to disable parallel tool calls:\n",
            "\n",
            "            ```python\n",
            "            ai_msg = model_with_tools.invoke(\n",
            "                \"What is the weather in LA and NY?\", parallel_tool_calls=False\n",
            "            )\n",
            "            ai_msg.tool_calls\n",
            "            ```\n",
            "\n",
            "            ```python\n",
            "            [\n",
            "                {\n",
            "                    \"name\": \"GetWeather\",\n",
            "                    \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            "                    \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n",
            "                }\n",
            "            ]\n",
            "            ```\n",
            "\n",
            "        Like other runtime parameters, `parallel_tool_calls` can be bound to a model\n",
            "        using `model.bind(parallel_tool_calls=False)` or during instantiation by\n",
            "        setting `model_kwargs`.\n",
            "\n",
            "        See `bind_tools` for more.\n",
            "\n",
            "    ??? info \"Built-in (server-side) tools\"\n",
            "\n",
            "        You can access [built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses)\n",
            "        supported by the OpenAI Responses API. See [LangChain docs](https://docs.langchain.com/oss/python/integrations/chat/openai#responses-api)\n",
            "        for more detail.\n",
            "\n",
            "        ```python\n",
            "        from langchain_openai import ChatOpenAI\n",
            "\n",
            "        model = ChatOpenAI(model=\"...\", output_version=\"responses/v1\")\n",
            "\n",
            "        tool = {\"type\": \"web_search\"}\n",
            "        model_with_tools = model.bind_tools([tool])\n",
            "\n",
            "        response = model_with_tools.invoke(\"What was a positive news story from today?\")\n",
            "        response.content\n",
            "        ```\n",
            "\n",
            "        ```python\n",
            "        [\n",
            "            {\n",
            "                \"type\": \"text\",\n",
            "                \"text\": \"Today, a heartwarming story emerged from ...\",\n",
            "                \"annotations\": [\n",
            "                    {\n",
            "                        \"end_index\": 778,\n",
            "                        \"start_index\": 682,\n",
            "                        \"title\": \"Title of story\",\n",
            "                        \"type\": \"url_citation\",\n",
            "                        \"url\": \"<url of story>\",\n",
            "                    }\n",
            "                ],\n",
            "            }\n",
            "        ]\n",
            "        ```\n",
            "\n",
            "        !!! version-added \"Added in `langchain-openai` 0.3.9\"\n",
            "\n",
            "        !!! version-added \"Added in `langchain-openai` 0.3.26: Updated `AIMessage` format\"\n",
            "            [`langchain-openai >= 0.3.26`](https://pypi.org/project/langchain-openai/#history)\n",
            "            allows users to opt-in to an updated `AIMessage` format when using the\n",
            "            Responses API. Setting `ChatOpenAI(..., output_version=\"responses/v1\")` will\n",
            "            format output from reasoning summaries, built-in tool invocations, and other\n",
            "            response items into the message's `content` field, rather than\n",
            "            `additional_kwargs`. We recommend this format for new applications.\n",
            "\n",
            "    ??? info \"Managing conversation state\"\n",
            "\n",
            "        OpenAI's Responses API supports management of [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses).\n",
            "        Passing in response IDs from previous messages will continue a conversational\n",
            "        thread.\n",
            "\n",
            "        ```python\n",
            "        from langchain_openai import ChatOpenAI\n",
            "\n",
            "        model = ChatOpenAI(\n",
            "            model=\"...\",\n",
            "            use_responses_api=True,\n",
            "            output_version=\"responses/v1\",\n",
            "        )\n",
            "        response = model.invoke(\"Hi, I'm Bob.\")\n",
            "        response.text\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        \"Hi Bob! How can I assist you today?\"\n",
            "        ```\n",
            "\n",
            "        ```python\n",
            "        second_response = model.invoke(\n",
            "            \"What is my name?\",\n",
            "            previous_response_id=response.response_metadata[\"id\"],\n",
            "        )\n",
            "        second_response.text\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        \"Your name is Bob. How can I help you today, Bob?\"\n",
            "        ```\n",
            "\n",
            "        !!! version-added \"Added in `langchain-openai` 0.3.9\"\n",
            "\n",
            "        !!! version-added \"Added in `langchain-openai` 0.3.26\"\n",
            "            You can also initialize `ChatOpenAI` with `use_previous_response_id`.\n",
            "            Input messages up to the most recent response will then be dropped from request\n",
            "            payloads, and `previous_response_id` will be set using the ID of the most\n",
            "            recent response.\n",
            "\n",
            "            ```python\n",
            "            model = ChatOpenAI(model=\"...\", use_previous_response_id=True)\n",
            "            ```\n",
            "\n",
            "    ??? info \"Reasoning output\"\n",
            "\n",
            "        OpenAI's Responses API supports [reasoning models](https://platform.openai.com/docs/guides/reasoning?api-mode=responses)\n",
            "        that expose a summary of internal reasoning processes.\n",
            "\n",
            "        ```python\n",
            "        from langchain_openai import ChatOpenAI\n",
            "\n",
            "        reasoning = {\n",
            "            \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
            "            \"summary\": \"auto\",  # 'detailed', 'auto', or None\n",
            "        }\n",
            "\n",
            "        model = ChatOpenAI(\n",
            "            model=\"...\", reasoning=reasoning, output_version=\"responses/v1\"\n",
            "        )\n",
            "        response = model.invoke(\"What is 3^3?\")\n",
            "\n",
            "        # Response text\n",
            "        print(f\"Output: {response.text}\")\n",
            "\n",
            "        # Reasoning summaries\n",
            "        for block in response.content:\n",
            "            if block[\"type\"] == \"reasoning\":\n",
            "                for summary in block[\"summary\"]:\n",
            "                    print(summary[\"text\"])\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        Output: 3¬≥ = 27\n",
            "        Reasoning: The user wants to know...\n",
            "        ```\n",
            "\n",
            "        !!! version-added \"Added in `langchain-openai` 0.3.26: Updated `AIMessage` format\"\n",
            "            [`langchain-openai >= 0.3.26`](https://pypi.org/project/langchain-openai/#history)\n",
            "            allows users to opt-in to an updated `AIMessage` format when using the\n",
            "            Responses API. Setting `ChatOpenAI(..., output_version=\"responses/v1\")` will\n",
            "            format output from reasoning summaries, built-in tool invocations, and other\n",
            "            response items into the message's `content` field, rather than\n",
            "            `additional_kwargs`. We recommend this format for new applications.\n",
            "\n",
            "    ??? info \"Structured output\"\n",
            "\n",
            "        ```python\n",
            "        from pydantic import BaseModel, Field\n",
            "\n",
            "\n",
            "        class Joke(BaseModel):\n",
            "            '''Joke to tell user.'''\n",
            "\n",
            "            setup: str = Field(description=\"The setup of the joke\")\n",
            "            punchline: str = Field(description=\"The punchline to the joke\")\n",
            "            rating: int | None = Field(\n",
            "                description=\"How funny the joke is, from 1 to 10\"\n",
            "            )\n",
            "\n",
            "\n",
            "        structured_model = model.with_structured_output(Joke)\n",
            "        structured_model.invoke(\"Tell me a joke about cats\")\n",
            "        ```\n",
            "\n",
            "        ```python\n",
            "        Joke(\n",
            "            setup=\"Why was the cat sitting on the computer?\",\n",
            "            punchline=\"To keep an eye on the mouse!\",\n",
            "            rating=None,\n",
            "        )\n",
            "        ```\n",
            "\n",
            "        See `with_structured_output` for more info.\n",
            "\n",
            "    ??? info \"JSON mode\"\n",
            "\n",
            "        ```python\n",
            "        json_model = model.bind(response_format={\"type\": \"json_object\"})\n",
            "        ai_msg = json_model.invoke(\n",
            "            \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n",
            "        )\n",
            "        ai_msg.content\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        '\\\\n{\\\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\\\n}'\n",
            "        ```\n",
            "\n",
            "    ??? info \"Image input\"\n",
            "\n",
            "        ```python\n",
            "        import base64\n",
            "        import httpx\n",
            "        from langchain.messages import HumanMessage\n",
            "\n",
            "        image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
            "        image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
            "        message = HumanMessage(\n",
            "            content=[\n",
            "                {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
            "                {\n",
            "                    \"type\": \"image_url\",\n",
            "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
            "                },\n",
            "            ]\n",
            "        )\n",
            "\n",
            "        ai_msg = model.invoke([message])\n",
            "        ai_msg.content\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n",
            "        ```\n",
            "\n",
            "    ??? info \"Token usage\"\n",
            "\n",
            "        ```python\n",
            "        ai_msg = model.invoke(messages)\n",
            "        ai_msg.usage_metadata\n",
            "\n",
            "        ```txt\n",
            "        {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
            "        ```\n",
            "\n",
            "        When streaming, set the `stream_usage` kwarg:\n",
            "\n",
            "        ```python\n",
            "        stream = model.stream(messages, stream_usage=True)\n",
            "        full = next(stream)\n",
            "        for chunk in stream:\n",
            "            full += chunk\n",
            "        full.usage_metadata\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
            "        ```\n",
            "\n",
            "    ??? info \"Logprobs\"\n",
            "\n",
            "        ```python\n",
            "        logprobs_model = model.bind(logprobs=True)\n",
            "        ai_msg = logprobs_model.invoke(messages)\n",
            "        ai_msg.response_metadata[\"logprobs\"]\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        {\n",
            "            \"content\": [\n",
            "                {\n",
            "                    \"token\": \"J\",\n",
            "                    \"bytes\": [74],\n",
            "                    \"logprob\": -4.9617593e-06,\n",
            "                    \"top_logprobs\": [],\n",
            "                },\n",
            "                {\n",
            "                    \"token\": \"'adore\",\n",
            "                    \"bytes\": [39, 97, 100, 111, 114, 101],\n",
            "                    \"logprob\": -0.25202933,\n",
            "                    \"top_logprobs\": [],\n",
            "                },\n",
            "                {\n",
            "                    \"token\": \" la\",\n",
            "                    \"bytes\": [32, 108, 97],\n",
            "                    \"logprob\": -0.20141791,\n",
            "                    \"top_logprobs\": [],\n",
            "                },\n",
            "                {\n",
            "                    \"token\": \" programmation\",\n",
            "                    \"bytes\": [\n",
            "                        32,\n",
            "                        112,\n",
            "                        114,\n",
            "                        111,\n",
            "                        103,\n",
            "                        114,\n",
            "                        97,\n",
            "                        109,\n",
            "                        109,\n",
            "                        97,\n",
            "                        116,\n",
            "                        105,\n",
            "                        111,\n",
            "                        110,\n",
            "                    ],\n",
            "                    \"logprob\": -1.9361265e-07,\n",
            "                    \"top_logprobs\": [],\n",
            "                },\n",
            "                {\n",
            "                    \"token\": \".\",\n",
            "                    \"bytes\": [46],\n",
            "                    \"logprob\": -1.2233183e-05,\n",
            "                    \"top_logprobs\": [],\n",
            "                },\n",
            "            ]\n",
            "        }\n",
            "        ```\n",
            "\n",
            "    ??? info \"Response metadata\"\n",
            "\n",
            "        ```python\n",
            "        ai_msg = model.invoke(messages)\n",
            "        ai_msg.response_metadata\n",
            "        ```\n",
            "\n",
            "        ```txt\n",
            "        {\n",
            "            \"token_usage\": {\n",
            "                \"completion_tokens\": 5,\n",
            "                \"prompt_tokens\": 28,\n",
            "                \"total_tokens\": 33,\n",
            "            },\n",
            "            \"model_name\": \"gpt-4o\",\n",
            "            \"system_fingerprint\": \"fp_319be4768e\",\n",
            "            \"finish_reason\": \"stop\",\n",
            "            \"logprobs\": None,\n",
            "        }\n",
            "        ```\n",
            "\n",
            "    ??? info \"Flex processing\"\n",
            "\n",
            "        OpenAI offers a variety of [service tiers](https://platform.openai.com/docs/guides/flex-processing?api-mode=responses).\n",
            "        The \"flex\" tier offers cheaper pricing for requests, with the trade-off that\n",
            "        responses may take longer and resources might not always be available.\n",
            "        This approach is best suited for non-critical tasks, including model testing,\n",
            "        data enhancement, or jobs that can be run asynchronously.\n",
            "\n",
            "        To use it, initialize the model with `service_tier=\"flex\"`:\n",
            "\n",
            "        ```python\n",
            "        from langchain_openai import ChatOpenAI\n",
            "\n",
            "        model = ChatOpenAI(model=\"...\", service_tier=\"flex\")\n",
            "        ```\n",
            "\n",
            "        Note that this is a beta feature that is only available for a subset of models.\n",
            "        See OpenAI [flex processing docs](https://platform.openai.com/docs/guides/flex-processing?api-mode=responses)\n",
            "        for more detail.\n",
            "\n",
            "    ??? info \"OpenAI-compatible APIs\"\n",
            "\n",
            "        `ChatOpenAI` can be used with OpenAI-compatible APIs like\n",
            "        [LM Studio](https://lmstudio.ai/), [vLLM](https://github.com/vllm-project/vllm),\n",
            "        [Ollama](https://ollama.com/), and others.\n",
            "\n",
            "        To use custom parameters specific to these providers, use the `extra_body` parameter.\n",
            "\n",
            "        !!! example \"LM Studio example with TTL (auto-eviction)\"\n",
            "\n",
            "            ```python\n",
            "            from langchain_openai import ChatOpenAI\n",
            "\n",
            "            model = ChatOpenAI(\n",
            "                base_url=\"http://localhost:1234/v1\",\n",
            "                api_key=\"lm-studio\",  # Can be any string\n",
            "                model=\"mlx-community/QwQ-32B-4bit\",\n",
            "                temperature=0,\n",
            "                extra_body={\n",
            "                    \"ttl\": 300\n",
            "                },  # Auto-evict model after 5 minutes of inactivity\n",
            "            )\n",
            "            ```\n",
            "\n",
            "        !!! example \"vLLM example with custom parameters\"\n",
            "\n",
            "            ```python\n",
            "            model = ChatOpenAI(\n",
            "                base_url=\"http://localhost:8000/v1\",\n",
            "                api_key=\"EMPTY\",\n",
            "                model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
            "                extra_body={\"use_beam_search\": True, \"best_of\": 4},\n",
            "            )\n",
            "            ```\n",
            "\n",
            "    ??? info \"`model_kwargs` vs `extra_body`\"\n",
            "\n",
            "        Use the correct parameter for different types of API arguments:\n",
            "\n",
            "        **Use `model_kwargs` for:**\n",
            "\n",
            "        - Standard OpenAI API parameters not explicitly defined as class parameters\n",
            "        - Parameters that should be flattened into the top-level request payload\n",
            "        - Examples: `max_completion_tokens`, `stream_options`, `modalities`, `audio`\n",
            "\n",
            "        ```python\n",
            "        # Standard OpenAI parameters\n",
            "        model = ChatOpenAI(\n",
            "            model=\"...\",\n",
            "            model_kwargs={\n",
            "                \"stream_options\": {\"include_usage\": True},\n",
            "                \"max_completion_tokens\": 300,\n",
            "                \"modalities\": [\"text\", \"audio\"],\n",
            "                \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},\n",
            "            },\n",
            "        )\n",
            "        ```\n",
            "\n",
            "        **Use `extra_body` for:**\n",
            "\n",
            "        - Custom parameters specific to OpenAI-compatible providers (vLLM, LM Studio,\n",
            "            OpenRouter, etc.)\n",
            "        - Parameters that need to be nested under `extra_body` in the request\n",
            "        - Any non-standard OpenAI API parameters\n",
            "\n",
            "        ```python\n",
            "        # Custom provider parameters\n",
            "        model = ChatOpenAI(\n",
            "            base_url=\"http://localhost:8000/v1\",\n",
            "            model=\"custom-model\",\n",
            "            extra_body={\n",
            "                \"use_beam_search\": True,  # vLLM parameter\n",
            "                \"best_of\": 4,  # vLLM parameter\n",
            "                \"ttl\": 300,  # LM Studio parameter\n",
            "            },\n",
            "        )\n",
            "        ```\n",
            "\n",
            "        **Key Differences:**\n",
            "\n",
            "        - `model_kwargs`: Parameters are **merged into top-level** request payload\n",
            "        - `extra_body`: Parameters are **nested under `extra_body`** key in request\n",
            "\n",
            "        !!! warning\n",
            "            Always use `extra_body` for custom parameters, **not** `model_kwargs`.\n",
            "            Using `model_kwargs` for non-OpenAI parameters will cause API errors.\n",
            "\n",
            "    ??? info \"Prompt caching optimization\"\n",
            "\n",
            "        For high-volume applications with repetitive prompts, use `prompt_cache_key`\n",
            "        per-invocation to improve cache hit rates and reduce costs:\n",
            "\n",
            "        ```python\n",
            "        model = ChatOpenAI(model=\"...\")\n",
            "\n",
            "        response = model.invoke(\n",
            "            messages,\n",
            "            prompt_cache_key=\"example-key-a\",  # Routes to same machine for cache hits\n",
            "        )\n",
            "\n",
            "        customer_response = model.invoke(messages, prompt_cache_key=\"example-key-b\")\n",
            "        support_response = model.invoke(messages, prompt_cache_key=\"example-key-c\")\n",
            "\n",
            "        # Dynamic cache keys based on context\n",
            "        cache_key = f\"example-key-{dynamic_suffix}\"\n",
            "        response = model.invoke(messages, prompt_cache_key=cache_key)\n",
            "        ```\n",
            "\n",
            "        Cache keys help ensure requests with the same prompt prefix are routed to\n",
            "        machines with existing cache, providing cost reduction and latency improvement on\n",
            "        cached tokens.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(create_agent.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u9enV8ZTHJb",
        "outputId": "29253782-798d-4f6b-b865-dc2c10e8f056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creates an agent graph that calls tools in a loop until a stopping condition is met.\n",
            "\n",
            "    For more details on using `create_agent`,\n",
            "    visit the [Agents](https://docs.langchain.com/oss/python/langchain/agents) docs.\n",
            "\n",
            "    Args:\n",
            "        model: The language model for the agent.\n",
            "\n",
            "            Can be a string identifier (e.g., `\"openai:gpt-4\"`) or a direct chat model\n",
            "            instance (e.g., [`ChatOpenAI`][langchain_openai.ChatOpenAI] or other another\n",
            "            [LangChain chat model](https://docs.langchain.com/oss/python/integrations/chat)).\n",
            "\n",
            "            For a full list of supported model strings, see\n",
            "            [`init_chat_model`][langchain.chat_models.init_chat_model(model_provider)].\n",
            "\n",
            "            !!! tip \"\"\n",
            "\n",
            "                See the [Models](https://docs.langchain.com/oss/python/langchain/models)\n",
            "                docs for more information.\n",
            "        tools: A list of tools, `dict`, or `Callable`.\n",
            "\n",
            "            If `None` or an empty list, the agent will consist of a model node without a\n",
            "            tool calling loop.\n",
            "\n",
            "\n",
            "            !!! tip \"\"\n",
            "\n",
            "                See the [Tools](https://docs.langchain.com/oss/python/langchain/tools)\n",
            "                docs for more information.\n",
            "        system_prompt: An optional system prompt for the LLM.\n",
            "\n",
            "            Can be a `str` (which will be converted to a `SystemMessage`) or a\n",
            "            `SystemMessage` instance directly. The system message is added to the\n",
            "            beginning of the message list when calling the model.\n",
            "        middleware: A sequence of middleware instances to apply to the agent.\n",
            "\n",
            "            Middleware can intercept and modify agent behavior at various stages.\n",
            "\n",
            "            !!! tip \"\"\n",
            "\n",
            "                See the [Middleware](https://docs.langchain.com/oss/python/langchain/middleware)\n",
            "                docs for more information.\n",
            "        response_format: An optional configuration for structured responses.\n",
            "\n",
            "            Can be a `ToolStrategy`, `ProviderStrategy`, or a Pydantic model class.\n",
            "\n",
            "            If provided, the agent will handle structured output during the\n",
            "            conversation flow.\n",
            "\n",
            "            Raw schemas will be wrapped in an appropriate strategy based on model\n",
            "            capabilities.\n",
            "\n",
            "            !!! tip \"\"\n",
            "\n",
            "                See the [Structured output](https://docs.langchain.com/oss/python/langchain/structured-output)\n",
            "                docs for more information.\n",
            "        state_schema: An optional `TypedDict` schema that extends `AgentState`.\n",
            "\n",
            "            When provided, this schema is used instead of `AgentState` as the base\n",
            "            schema for merging with middleware state schemas. This allows users to\n",
            "            add custom state fields without needing to create custom middleware.\n",
            "\n",
            "            Generally, it's recommended to use `state_schema` extensions via middleware\n",
            "            to keep relevant extensions scoped to corresponding hooks / tools.\n",
            "        context_schema: An optional schema for runtime context.\n",
            "        checkpointer: An optional checkpoint saver object.\n",
            "\n",
            "            Used for persisting the state of the graph (e.g., as chat memory) for a\n",
            "            single thread (e.g., a single conversation).\n",
            "        store: An optional store object.\n",
            "\n",
            "            Used for persisting data across multiple threads (e.g., multiple\n",
            "            conversations / users).\n",
            "        interrupt_before: An optional list of node names to interrupt before.\n",
            "\n",
            "            Useful if you want to add a user confirmation or other interrupt\n",
            "            before taking an action.\n",
            "        interrupt_after: An optional list of node names to interrupt after.\n",
            "\n",
            "            Useful if you want to return directly or run additional processing\n",
            "            on an output.\n",
            "        debug: Whether to enable verbose logging for graph execution.\n",
            "\n",
            "            When enabled, prints detailed information about each node execution, state\n",
            "            updates, and transitions during agent runtime. Useful for debugging\n",
            "            middleware behavior and understanding agent execution flow.\n",
            "        name: An optional name for the `CompiledStateGraph`.\n",
            "\n",
            "            This name will be automatically used when adding the agent graph to\n",
            "            another graph as a subgraph node - particularly useful for building\n",
            "            multi-agent systems.\n",
            "        cache: An optional `BaseCache` instance to enable caching of graph execution.\n",
            "\n",
            "    Returns:\n",
            "        A compiled `StateGraph` that can be used for chat interactions.\n",
            "\n",
            "    The agent node calls the language model with the messages list (after applying\n",
            "    the system prompt). If the resulting [`AIMessage`][langchain.messages.AIMessage]\n",
            "    contains `tool_calls`, the graph will then call the tools. The tools node executes\n",
            "    the tools and adds the responses to the messages list as\n",
            "    [`ToolMessage`][langchain.messages.ToolMessage] objects. The agent node then calls\n",
            "    the language model again. The process repeats until no more `tool_calls` are present\n",
            "    in the response. The agent then returns the full list of messages.\n",
            "\n",
            "    Example:\n",
            "        ```python\n",
            "        from langchain.agents import create_agent\n",
            "\n",
            "\n",
            "        def check_weather(location: str) -> str:\n",
            "            '''Return the weather forecast for the specified location.'''\n",
            "            return f\"It's always sunny in {location}\"\n",
            "\n",
            "\n",
            "        graph = create_agent(\n",
            "            model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
            "            tools=[check_weather],\n",
            "            system_prompt=\"You are a helpful assistant\",\n",
            "        )\n",
            "        inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
            "        for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n",
            "            print(chunk)\n",
            "        ```\n",
            "    \n"
          ]
        }
      ]
    }
  ]
}